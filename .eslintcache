[{"E:\\repos\\react-speech-to-text\\src\\index.js":"1","E:\\repos\\react-speech-to-text\\src\\App.tsx":"2","E:\\repos\\react-speech-to-text\\src\\Hooks\\index.tsx":"3","E:\\repos\\react-speech-to-text\\src\\Hooks\\recorderHelpers.js":"4","E:\\repos\\react-speech-to-text\\src\\Hooks\\recorder.js":"5"},{"size":153,"mtime":1666286526287,"results":"6","hashOfConfig":"7"},{"size":1538,"mtime":1667421534284,"results":"8","hashOfConfig":"7"},{"size":12105,"mtime":1667421664872,"results":"9","hashOfConfig":"7"},{"size":2315,"mtime":1667422784247,"results":"10","hashOfConfig":"7"},{"size":8788,"mtime":1666286526287,"results":"11","hashOfConfig":"7"},{"filePath":"12","messages":"13","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"14"},"1tynzxm",{"filePath":"15","messages":"16","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"17"},{"filePath":"18","messages":"19","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"20","usedDeprecatedRules":"17"},{"filePath":"21","messages":"22","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},{"filePath":"23","messages":"24","errorCount":0,"warningCount":3,"fixableErrorCount":0,"fixableWarningCount":0,"source":"25","usedDeprecatedRules":"14"},"E:\\repos\\react-speech-to-text\\src\\index.js",[],["26","27"],"E:\\repos\\react-speech-to-text\\src\\App.tsx",[],["28","29"],"E:\\repos\\react-speech-to-text\\src\\Hooks\\index.tsx",["30"],"import { useState, useEffect, useRef } from 'react';\r\nimport Hark from 'hark';\r\nimport { startRecording, stopRecording } from './recorderHelpers';\r\n\r\n// https://cloud.google.com/speech-to-text/docs/reference/rest/v1/RecognitionConfig\r\nimport { GoogleCloudRecognitionConfig } from './GoogleCloudRecognitionConfig';\r\n\r\n// https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognition\r\nexport interface SpeechRecognitionProperties {\r\n  // continuous: do not pass continuous here, instead pass it as a param to the hook\r\n  grammars?: SpeechGrammarList;\r\n  interimResults?: boolean;\r\n  lang?: string;\r\n  maxAlternatives?: number;\r\n}\r\n\r\nconst isEdgeChromium = navigator.userAgent.indexOf('Edg/') !== -1;\r\n\r\ninterface BraveNavigator extends Navigator {\r\n  brave: {\r\n    isBrave: () => Promise<boolean>;\r\n  };\r\n}\r\n\r\nconst AudioContext = window.AudioContext || (window as any).webkitAudioContext;\r\n\r\nconst SpeechRecognition =\r\n  window.SpeechRecognition || (window as any).webkitSpeechRecognition;\r\n\r\nlet recognition: SpeechRecognition | null;\r\n\r\nexport type ResultType = {\r\n  speechBlob?: Blob;\r\n  timestamp: number;\r\n  transcript: string;\r\n  confidence: number;\r\n};\r\n\r\n// Set recognition back to null for brave browser due to promise resolving\r\n// after the conditional on line 31\r\nif ((navigator as BraveNavigator).brave) {\r\n  (navigator as BraveNavigator).brave.isBrave().then((bool) => {\r\n    if (bool) recognition = null;\r\n  });\r\n}\r\n\r\n// Chromium browsers will have the SpeechRecognition method\r\n// but do not implement the functionality due to google wanting ðŸ’°\r\n// this covers new Edge and line 22 covers Brave, the two most popular non-chrome chromium browsers\r\nif (!isEdgeChromium && SpeechRecognition) {\r\n  recognition = new SpeechRecognition();\r\n}\r\n\r\nexport interface UseSpeechToTextTypes {\r\n  continuous?: boolean;\r\n  fullAudio?: boolean;\r\n  crossBrowser?: boolean;\r\n  googleApiKey?: string;\r\n  googleCloudRecognitionConfig?: GoogleCloudRecognitionConfig;\r\n  onStartSpeaking?: () => any;\r\n  onStoppedSpeaking?: () => any;\r\n  speechRecognitionProperties?: SpeechRecognitionProperties;\r\n  timeout?: number;\r\n  useLegacyResults?: boolean;\r\n  useOnlyGoogleCloud?: boolean;\r\n  threshold?: number;\r\n}\r\n\r\nexport default function useSpeechToText({\r\n  continuous,\r\n  fullAudio,\r\n  crossBrowser,\r\n  googleApiKey,\r\n  googleCloudRecognitionConfig,\r\n  onStartSpeaking,\r\n  onStoppedSpeaking,\r\n  speechRecognitionProperties = { interimResults: true },\r\n  timeout = 10000,\r\n  useOnlyGoogleCloud = false,\r\n  useLegacyResults = true,\r\n  threshold = 0.8\r\n}: UseSpeechToTextTypes) {\r\n  const [isRecording, setIsRecording] = useState(false);\r\n\r\n  const audioContextRef = useRef<AudioContext>();\r\n\r\n  const [legacyResults, setLegacyResults] = useState<string[]>([]);\r\n  const [results, setResults] = useState<ResultType[]>([]);\r\n\r\n  const [interimResult, setInterimResult] = useState<string | undefined>();\r\n  const [error, setError] = useState('');\r\n\r\n  const timeoutId = useRef<number>();\r\n  const [vol, setVol] = useState<number>();\r\n  const mediaStream = useRef<MediaStream>();\r\n\r\n  useEffect(() => {\r\n    if (!crossBrowser && !recognition) {\r\n      setError('Speech Recognition API is only available on Chrome');\r\n    }\r\n\r\n    if (!navigator?.mediaDevices?.getUserMedia) {\r\n      setError('getUserMedia is not supported on this device/browser :(');\r\n    }\r\n\r\n    if ((crossBrowser || useOnlyGoogleCloud) && !googleApiKey) {\r\n      console.error(\r\n        'No google cloud API key was passed, google API will not be able to process speech'\r\n      );\r\n    }\r\n\r\n    if (!audioContextRef.current) {\r\n      audioContextRef.current = new AudioContext();\r\n    }\r\n\r\n    if (useLegacyResults) {\r\n      console.warn(\r\n        'react-hook-speech-to-text is using legacy results, pass useLegacyResults: false to the hook to use the new array of objects results. Legacy array of strings results will be removed in a future version.'\r\n      );\r\n    }\r\n  }, []);\r\n\r\n  // Chrome Speech Recognition API:\r\n  // Only supported on Chrome browsers\r\n  const chromeSpeechRecognition = () => {\r\n    if (recognition) {\r\n      // Continuous recording after stopped speaking event\r\n      if (continuous) recognition.continuous = true;\r\n\r\n      const { grammars, interimResults, lang, maxAlternatives } =\r\n        speechRecognitionProperties || {};\r\n\r\n      if (grammars) recognition.grammars = grammars;\r\n      if (lang) recognition.lang = lang;\r\n\r\n      recognition.interimResults = interimResults || false;\r\n      recognition.maxAlternatives = maxAlternatives || 1;\r\n\r\n      // start recognition\r\n      recognition.start();\r\n\r\n      // speech successfully translated into text\r\n      recognition.onresult = (e) => {\r\n        const result = e.results[e.results.length - 1];\r\n        const { transcript, confidence } = result[0];\r\n\r\n        const timestamp = Math.floor(Date.now() / 1000);\r\n\r\n        // Allows for realtime speech result UI feedback\r\n        if (interimResults) {\r\n          if (result.isFinal) {\r\n            setInterimResult(undefined);\r\n            setResults((prevResults) => [\r\n              ...prevResults,\r\n              { transcript, timestamp, confidence }\r\n            ]);\r\n            setLegacyResults((prevResults) => [...prevResults, transcript]);\r\n          } else {\r\n            let concatTranscripts = '';\r\n\r\n            // If continuous: e.results will include previous speech results: need to start loop at the current event resultIndex for proper concatenation\r\n            for (let i = e.resultIndex; i < e.results.length; i++) {\r\n              concatTranscripts += e.results[i][0].transcript;\r\n            }\r\n\r\n            setInterimResult(concatTranscripts);\r\n          }\r\n        } else {\r\n          setResults((prevResults) => [\r\n            ...prevResults,\r\n            { transcript, timestamp, confidence }\r\n          ]);\r\n          setLegacyResults((prevResults) => [...prevResults, transcript]);\r\n        }\r\n      };\r\n\r\n      recognition.onaudiostart = () => setIsRecording(true);\r\n\r\n      // Audio stopped recording or timed out.\r\n      // Chrome speech auto times-out if no speech after a while\r\n      recognition.onend = () => {\r\n        setIsRecording(false);\r\n      };\r\n    }\r\n  };\r\n\r\n  const startSpeechToText = async () => {\r\n    if (!useOnlyGoogleCloud && recognition) {\r\n      chromeSpeechRecognition();\r\n      return;\r\n    }\r\n\r\n    if (!crossBrowser && !useOnlyGoogleCloud) {\r\n      return;\r\n    }\r\n\r\n    // Resume audio context due to google auto play policy\r\n    // https://developers.google.com/web/updates/2017/09/autoplay-policy-changes#webaudio\r\n    if (audioContextRef.current?.state === 'suspended') {\r\n      audioContextRef.current?.resume();\r\n    }\r\n\r\n    const stream = await startRecording({\r\n      setVol: setVol,\r\n      errHandler: () => setError('Microphone permission was denied'),\r\n      audioContext: audioContextRef.current as AudioContext\r\n    });\r\n\r\n    setIsRecording(true);\r\n\r\n    // Stop recording if timeout\r\n    if (timeout) {\r\n      clearTimeout(timeoutId.current);\r\n      handleRecordingTimeout();\r\n    }\r\n\r\n    // stop previous mediaStream track if exists\r\n    if (mediaStream.current) {\r\n      stopMediaStream();\r\n    }\r\n\r\n    // Clones stream to fix hark bug on Safari\r\n    mediaStream.current = stream.clone();\r\n\r\n    const speechEvents = Hark(mediaStream.current, {\r\n      audioContext: audioContextRef.current as AudioContext\r\n    });\r\n\r\n    speechEvents.on('speaking', () => {\r\n      if (onStartSpeaking) onStartSpeaking();\r\n\r\n      // Clear previous recording timeout on every speech event\r\n      clearTimeout(timeoutId.current);\r\n    });\r\n\r\n    speechEvents.on('stopped_speaking', () => {\r\n      if (fullAudio) return;\r\n      if (onStoppedSpeaking) onStoppedSpeaking();\r\n\r\n      // Stops current recording and sends audio string to google cloud.\r\n      // recording will start again after google cloud api\r\n      // call if `continuous` prop is true. Until the api result\r\n      // returns, technically the microphone is not being captured again\r\n      stopRecording({\r\n        exportWAV: true,\r\n        wavCallback: (blob) =>\r\n          handleBlobToBase64({ blob, continuous: continuous || false })\r\n      });\r\n    });\r\n  };\r\n\r\n  const stopSpeechToText = () => {\r\n    if (recognition && !useOnlyGoogleCloud) {\r\n      recognition.stop();\r\n    } else {\r\n      setIsRecording(false);\r\n      stopMediaStream();\r\n      stopRecording({\r\n        exportWAV: true,\r\n        wavCallback: (blob) => handleBlobToBase64({ blob, continuous: false })\r\n      });\r\n    }\r\n  };\r\n\r\n  const stopSpeechToTextNoData = () => {\r\n      setIsRecording(false);\r\n      stopMediaStream();\r\n      stopRecording({ exportWAV: false });\r\n  };\r\n\r\n  const handleRecordingTimeout = () => {\r\n    timeoutId.current = window.setTimeout(() => {\r\n      setIsRecording(false);\r\n      stopMediaStream();\r\n      stopRecording({ exportWAV: false });\r\n    }, timeout);\r\n  };\r\n\r\n  const handleBlobToBase64 = ({\r\n    blob,\r\n    continuous\r\n  }: {\r\n    blob: Blob;\r\n    continuous: boolean;\r\n  }) => {\r\n    const reader = new FileReader();\r\n    reader.readAsDataURL(blob);\r\n\r\n    reader.onloadend = async () => {\r\n      const base64data = reader.result as string;\r\n\r\n      let sampleRate = audioContextRef.current?.sampleRate;\r\n\r\n      // Google only accepts max 48000 sample rate: if\r\n      // greater recorder js will down-sample to 48000\r\n      if (sampleRate && sampleRate > 48000) {\r\n        sampleRate = 48000;\r\n      }\r\n\r\n      const audio = { content: '' };\r\n\r\n      const config: GoogleCloudRecognitionConfig = {\r\n        encoding: 'LINEAR16',\r\n        languageCode: 'en-US',\r\n        sampleRateHertz: sampleRate,\r\n        ...googleCloudRecognitionConfig\r\n      };\r\n\r\n      const data = {\r\n        config,\r\n        audio\r\n      };\r\n\r\n      // Gets raw base 64 string data\r\n      audio.content = base64data.substr(base64data.indexOf(',') + 1);\r\n\r\n      const googleCloudRes = await fetch(\r\n        `https://speech.googleapis.com/v1p1beta1/speech:recognize?key=${googleApiKey}`,\r\n        {\r\n          method: 'POST',\r\n          body: JSON.stringify(data)\r\n        }\r\n      );\r\n\r\n      const googleCloudJson = await googleCloudRes.json();\r\n\r\n      // Update results state with transcribed text\r\n      if (googleCloudJson.results?.length > 0) {\r\n\r\n        if (fullAudio) {\r\n          const results = (googleCloudJson.results as GoogleResults[]).reduce((prev: Alternative[], curr: GoogleResults) => {\r\n              if (curr.alternatives?.length > 0) {\r\n                const highestConfidenceAlternative = curr.alternatives.sort((a,b) => b.confidence - a.confidence)[0];\r\n                return prev.concat(highestConfidenceAlternative);\r\n              }\r\n              return prev;\r\n          }, []).reduce((prev, curr) => prev + curr.transcript, '');\r\n\r\n        const res = {\r\n          speechBlob: blob,\r\n          transcript: results.toLocaleLowerCase(),\r\n          timestamp: Math.floor(Date.now() / 1000),\r\n          confidence: 1\r\n        };\r\n        setResults([res]);\r\n\r\n        } else {\r\n          const res = googleCloudJson.results[0].alternatives.map((alternative: any) => ({\r\n            speechBlob: blob,\r\n            transcript: alternative.transcript,\r\n            timestamp: Math.floor(Date.now() / 1000),\r\n            confidence: alternative.confidence\r\n          }));\r\n\r\n          res.filter((result: any) => result.confidence >= threshold);\r\n\r\n          setResults(res);\r\n        }\r\n      }\r\n\r\n      if (continuous) {\r\n        startSpeechToText();\r\n      } else {\r\n        stopMediaStream();\r\n        setIsRecording(false);\r\n      }\r\n    };\r\n  };\r\n\r\n  const stopMediaStream = () => {\r\n    mediaStream.current?.getAudioTracks()[0].stop();\r\n  };\r\n\r\n  return {\r\n    error,\r\n    interimResult,\r\n    isRecording,\r\n    results: useLegacyResults ? legacyResults : results,\r\n    vol: vol,\r\n    setResults,\r\n    startSpeechToText,\r\n    stopSpeechToText,\r\n    stopSpeechToTextNoData\r\n  };\r\n}\r\n\r\ntype Alternative = {\r\n    transcript: string,\r\n    confidence: number\r\n}\r\n\r\ntype GoogleResults = {\r\n  alternatives: Alternative[],\r\n  resultEndTime: string,\r\n  languageCode: string\r\n}","E:\\repos\\react-speech-to-text\\src\\Hooks\\recorderHelpers.js",[],"E:\\repos\\react-speech-to-text\\src\\Hooks\\recorder.js",["31","32","33"],"import InlineWorker from 'inline-worker';\r\n\r\nexport class Recorder {\r\n  constructor(source, cfg) {\r\n    this.config = {\r\n      bufferLen: 4096,\r\n      numChannels: 1,\r\n      mimeType: 'audio/wav',\r\n      ...cfg\r\n    };\r\n    this.recording = false;\r\n    this.callbacks = {\r\n      getBuffer: [],\r\n      exportWAV: []\r\n    };\r\n    this.context = source.context;\r\n    this.node = (\r\n      this.context.createScriptProcessor || this.context.createJavaScriptNode\r\n    ).call(\r\n      this.context,\r\n      this.config.bufferLen,\r\n      this.config.numChannels,\r\n      this.config.numChannels\r\n    );\r\n\r\n    this.node.onaudioprocess = (e) => {\r\n      if (!this.recording) return;\r\n\r\n      var buffer = [];\r\n      for (var channel = 0; channel < this.config.numChannels; channel++) {\r\n        buffer.push(e.inputBuffer.getChannelData(channel));\r\n      }\r\n      this.worker.postMessage({\r\n        command: 'record',\r\n        buffer: buffer\r\n      });\r\n    };\r\n\r\n    source.connect(this.node);\r\n    this.node.connect(this.context.destination); //this should not be necessary\r\n\r\n    let self = {};\r\n    this.worker = new InlineWorker(function () {\r\n      let recLength = 0,\r\n        recBuffers = [],\r\n        sampleRate,\r\n        numChannels;\r\n\r\n      this.onmessage = function (e) {\r\n        switch (e.data.command) {\r\n          case 'init':\r\n            init(e.data.config);\r\n            break;\r\n          case 'record':\r\n            record(e.data.buffer);\r\n            break;\r\n          case 'exportWAV':\r\n            exportWAV(e.data.type);\r\n            break;\r\n          case 'getBuffer':\r\n            getBuffer();\r\n            break;\r\n          case 'clear':\r\n            clear();\r\n            break;\r\n        }\r\n      };\r\n\r\n      let newSampleRate;\r\n\r\n      function init(config) {\r\n        sampleRate = config.sampleRate;\r\n        numChannels = config.numChannels;\r\n        initBuffers();\r\n\r\n        if (sampleRate > 48000) {\r\n          newSampleRate = 48000;\r\n        } else {\r\n          newSampleRate = sampleRate;\r\n        }\r\n      }\r\n\r\n      function record(inputBuffer) {\r\n        for (var channel = 0; channel < numChannels; channel++) {\r\n          recBuffers[channel].push(inputBuffer[channel]);\r\n        }\r\n        recLength += inputBuffer[0].length;\r\n      }\r\n\r\n      function exportWAV(type) {\r\n        let buffers = [];\r\n        for (let channel = 0; channel < numChannels; channel++) {\r\n          buffers.push(mergeBuffers(recBuffers[channel], recLength));\r\n        }\r\n        let interleaved;\r\n        if (numChannels === 2) {\r\n          interleaved = interleave(buffers[0], buffers[1]);\r\n        } else {\r\n          interleaved = buffers[0];\r\n        }\r\n\r\n        // converts sample rate to 48000 if higher than 48000\r\n        let downSampledBuffer = downSampleBuffer(interleaved, newSampleRate);\r\n\r\n        let dataview = encodeWAV(downSampledBuffer);\r\n        let audioBlob = new Blob([dataview], { type: type });\r\n\r\n        this.postMessage({ command: 'exportWAV', data: audioBlob });\r\n      }\r\n\r\n      function getBuffer() {\r\n        let buffers = [];\r\n        for (let channel = 0; channel < numChannels; channel++) {\r\n          buffers.push(mergeBuffers(recBuffers[channel], recLength));\r\n        }\r\n        this.postMessage({ command: 'getBuffer', data: buffers });\r\n      }\r\n\r\n      function clear() {\r\n        recLength = 0;\r\n        recBuffers = [];\r\n        initBuffers();\r\n      }\r\n\r\n      function initBuffers() {\r\n        for (let channel = 0; channel < numChannels; channel++) {\r\n          recBuffers[channel] = [];\r\n        }\r\n      }\r\n\r\n      function mergeBuffers(recBuffers, recLength) {\r\n        let result = new Float32Array(recLength);\r\n        let offset = 0;\r\n        for (let i = 0; i < recBuffers.length; i++) {\r\n          result.set(recBuffers[i], offset);\r\n          offset += recBuffers[i].length;\r\n        }\r\n        return result;\r\n      }\r\n\r\n      function interleave(inputL, inputR) {\r\n        let length = inputL.length + inputR.length;\r\n        let result = new Float32Array(length);\r\n\r\n        let index = 0,\r\n          inputIndex = 0;\r\n\r\n        while (index < length) {\r\n          result[index++] = inputL[inputIndex];\r\n          result[index++] = inputR[inputIndex];\r\n          inputIndex++;\r\n        }\r\n        return result;\r\n      }\r\n\r\n      function floatTo16BitPCM(output, offset, input) {\r\n        for (let i = 0; i < input.length; i++, offset += 2) {\r\n          let s = Math.max(-1, Math.min(1, input[i]));\r\n          output.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);\r\n        }\r\n      }\r\n\r\n      function writeString(view, offset, string) {\r\n        for (let i = 0; i < string.length; i++) {\r\n          view.setUint8(offset + i, string.charCodeAt(i));\r\n        }\r\n      }\r\n\r\n      // Down sample buffer before WAV encoding\r\n      function downSampleBuffer(buffer, rate) {\r\n        if (rate == sampleRate) {\r\n          return buffer;\r\n        }\r\n        if (rate > sampleRate) {\r\n          throw 'downsampling rate show be smaller than original sample rate';\r\n        }\r\n        var sampleRateRatio = sampleRate / rate;\r\n        var newLength = Math.round(buffer.length / sampleRateRatio);\r\n        var result = new Float32Array(newLength);\r\n        var offsetResult = 0;\r\n        var offsetBuffer = 0;\r\n        while (offsetResult < result.length) {\r\n          var nextOffsetBuffer = Math.round(\r\n            (offsetResult + 1) * sampleRateRatio\r\n          );\r\n          // Use average value of skipped samples\r\n          var accum = 0,\r\n            count = 0;\r\n          for (\r\n            var i = offsetBuffer;\r\n            i < nextOffsetBuffer && i < buffer.length;\r\n            i++\r\n          ) {\r\n            accum += buffer[i];\r\n            count++;\r\n          }\r\n          result[offsetResult] = accum / count;\r\n          // Or you can simply get rid of the skipped samples:\r\n          // result[offsetResult] = buffer[nextOffsetBuffer];\r\n          offsetResult++;\r\n          offsetBuffer = nextOffsetBuffer;\r\n        }\r\n        return result;\r\n      }\r\n\r\n      function encodeWAV(samples) {\r\n        let buffer = new ArrayBuffer(44 + samples.length * 2);\r\n        let view = new DataView(buffer);\r\n\r\n        /* RIFF identifier */\r\n        writeString(view, 0, 'RIFF');\r\n        /* RIFF chunk length */\r\n        view.setUint32(4, 36 + samples.length * 2, true);\r\n        /* RIFF type */\r\n        writeString(view, 8, 'WAVE');\r\n        /* format chunk identifier */\r\n        writeString(view, 12, 'fmt ');\r\n        /* format chunk length */\r\n        view.setUint32(16, 16, true);\r\n        /* sample format (raw) */\r\n        view.setUint16(20, 1, true);\r\n        /* channel count */\r\n        view.setUint16(22, numChannels, true);\r\n        /* sample rate */\r\n        view.setUint32(24, newSampleRate, true);\r\n        /* byte rate (sample rate * block align) */\r\n        view.setUint32(28, newSampleRate * 4, true);\r\n        /* block align (channel count * bytes per sample) */\r\n        view.setUint16(32, numChannels * 2, true);\r\n        /* bits per sample */\r\n        view.setUint16(34, 16, true);\r\n        /* data chunk identifier */\r\n        writeString(view, 36, 'data');\r\n        /* data chunk length */\r\n        view.setUint32(40, samples.length * 2, true);\r\n\r\n        floatTo16BitPCM(view, 44, samples);\r\n\r\n        return view;\r\n      }\r\n    }, self);\r\n\r\n    this.worker.postMessage({\r\n      command: 'init',\r\n      config: {\r\n        sampleRate: this.context.sampleRate,\r\n        numChannels: this.config.numChannels\r\n      }\r\n    });\r\n\r\n    this.worker.onmessage = (e) => {\r\n      let cb = this.callbacks[e.data.command].pop();\r\n      if (typeof cb == 'function') {\r\n        cb(e.data.data);\r\n      }\r\n    };\r\n  }\r\n\r\n  record() {\r\n    this.recording = true;\r\n  }\r\n\r\n  stop() {\r\n    this.recording = false;\r\n  }\r\n\r\n  clear() {\r\n    this.worker.postMessage({ command: 'clear' });\r\n  }\r\n\r\n  getBuffer(cb) {\r\n    cb = cb || this.config.callback;\r\n    if (!cb) throw new Error('Callback not set');\r\n\r\n    this.callbacks.getBuffer.push(cb);\r\n\r\n    this.worker.postMessage({ command: 'getBuffer' });\r\n  }\r\n\r\n  exportWAV(cb, mimeType) {\r\n    mimeType = mimeType || this.config.mimeType;\r\n    cb = cb || this.config.callback;\r\n    if (!cb) throw new Error('Callback not set');\r\n\r\n    this.callbacks.exportWAV.push(cb);\r\n\r\n    this.worker.postMessage({\r\n      command: 'exportWAV',\r\n      type: mimeType\r\n    });\r\n  }\r\n\r\n  static forceDownload(blob, filename) {\r\n    let url = (window.URL || window.webkitURL).createObjectURL(blob);\r\n    let link = window.document.createElement('a');\r\n    link.href = url;\r\n    link.download = filename || 'output.wav';\r\n    let click = document.createEvent('Event');\r\n    click.initEvent('click', true, true);\r\n    link.dispatchEvent(click);\r\n  }\r\n}\r\n\r\nexport default Recorder;\r\n",{"ruleId":"34","replacedBy":"35"},{"ruleId":"36","replacedBy":"37"},{"ruleId":"34","replacedBy":"35"},{"ruleId":"36","replacedBy":"37"},{"ruleId":"38","severity":1,"message":"39","line":121,"column":6,"nodeType":"40","endLine":121,"endColumn":8,"suggestions":"41"},{"ruleId":"42","severity":1,"message":"43","line":50,"column":9,"nodeType":"44","messageId":"45","endLine":66,"endColumn":10},{"ruleId":"46","severity":1,"message":"47","line":171,"column":18,"nodeType":"48","messageId":"49","endLine":171,"endColumn":20},{"ruleId":"50","severity":1,"message":"51","line":175,"column":11,"nodeType":"52","messageId":"53","endLine":175,"endColumn":79},"no-native-reassign",["54"],"no-negated-in-lhs",["55"],"react-hooks/exhaustive-deps","React Hook useEffect has missing dependencies: 'crossBrowser', 'googleApiKey', 'useLegacyResults', and 'useOnlyGoogleCloud'. Either include them or remove the dependency array.","ArrayExpression",["56"],"default-case","Expected a default case.","SwitchStatement","missingDefaultCase","eqeqeq","Expected '===' and instead saw '=='.","BinaryExpression","unexpected","no-throw-literal","Expected an error object to be thrown.","ThrowStatement","object","no-global-assign","no-unsafe-negation",{"desc":"57","fix":"58"},"Update the dependencies array to be: [crossBrowser, googleApiKey, useLegacyResults, useOnlyGoogleCloud]",{"range":"59","text":"60"},[3935,3937],"[crossBrowser, googleApiKey, useLegacyResults, useOnlyGoogleCloud]"]